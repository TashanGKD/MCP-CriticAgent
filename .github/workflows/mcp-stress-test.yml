name: ğŸš€ MCP Tools Stress Testing

on:
  workflow_dispatch:
    inputs:
      test_limit:
        description: 'æµ‹è¯•å·¥å…·æ•°é‡é™åˆ¶'
        required: false
        default: '50'
        type: string
      deployment_method:
        description: 'éƒ¨ç½²æ–¹å¼è¿‡æ»¤'
        required: false
        default: 'npx'
        type: choice
        options:
        - 'npx'
        - 'pip'
        - 'all'
      timeout_per_tool:
        description: 'å•ä¸ªå·¥å…·æµ‹è¯•è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'
        required: false
        default: '300'
        type: string
      smart_mode:
        description: 'å¯ç”¨AIæ™ºèƒ½æµ‹è¯•'
        required: false
        default: 'true'
        type: boolean
      category_filter:
        description: 'æŒ‰ç±»åˆ«è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰'
        required: false
        default: ''
        type: string
  schedule:
    # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹è‡ªåŠ¨æ‰§è¡Œå‹æµ‹ï¼ˆUTCæ—¶é—´ï¼‰
    - cron: '0 2 * * 0'

jobs:
  stress-test:
    permissions:
      contents: read
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    
    env:
      # AIæ¨¡å‹é…ç½®
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
      OPENAI_MODEL: ${{ secrets.OPENAI_MODEL }}
      DASHSCOPE_API_KEY: ${{ secrets.DASHSCOPE_API_KEY }}
      DASHSCOPE_BASE_URL: ${{ secrets.DASHSCOPE_BASE_URL }}
      DASHSCOPE_MODEL: ${{ secrets.DASHSCOPE_MODEL }}
      
      # Supabaseæ•°æ®åº“é…ç½®
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '22'
        
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.local/bin" >> $GITHUB_PATH
      
    - name: Install dependencies
      run: uv sync
    
    - name: ğŸ¯ Generate Test Target List
      id: generate_targets
      run: |
        echo "ğŸ” ç”Ÿæˆå‹æµ‹ç›®æ ‡åˆ—è¡¨..."
        
        # è®¾ç½®å‚æ•°
        TEST_LIMIT="${{ github.event.inputs.test_limit || '50' }}"
        DEPLOYMENT_METHOD="${{ github.event.inputs.deployment_method || 'npx' }}"
        CATEGORY_FILTER="${{ github.event.inputs.category_filter || '' }}"
        
        echo "ğŸ“Š å‹æµ‹é…ç½®:"
        echo "  - æµ‹è¯•é™åˆ¶: $TEST_LIMIT ä¸ªå·¥å…·"
        echo "  - éƒ¨ç½²æ–¹å¼: $DEPLOYMENT_METHOD"
        echo "  - ç±»åˆ«è¿‡æ»¤: ${CATEGORY_FILTER:-'æ— '}"
        
        # ç”Ÿæˆæµ‹è¯•ç›®æ ‡åˆ—è¡¨
        cat > generate_targets.py << 'EOF'
        #!/usr/bin/env python3
        import sys
        import os
        sys.path.insert(0, '.')
        
        from src.utils.csv_parser import get_mcp_parser
        
        def main():
            parser = get_mcp_parser()
            tools = parser.get_all_tools()
            
            # ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
            test_limit = int(os.getenv('TEST_LIMIT', 50))
            deployment_method = os.getenv('DEPLOYMENT_METHOD', 'npx').lower()
            category_filter = os.getenv('CATEGORY_FILTER', '').lower()
            
            print(f"ğŸ“¦ æ€»å·¥å…·æ•°: {len(tools)}")
            
            # è¿‡æ»¤æ¡ä»¶
            filtered_tools = []
            for tool in tools:
                # éƒ¨ç½²æ–¹å¼è¿‡æ»¤
                if deployment_method != 'all':
                    if deployment_method == 'npx' and tool.deployment_method != 'npx':
                        continue
                    elif deployment_method == 'pip' and tool.deployment_method != 'pip':
                        continue
                
                # ç±»åˆ«è¿‡æ»¤
                if category_filter and category_filter not in tool.category.lower():
                    continue
                
                # å¿…é¡»æœ‰åŒ…åæ‰èƒ½æµ‹è¯•
                if not tool.package_name:
                    continue
                    
                filtered_tools.append(tool)
            
            print(f"ğŸ¯ è¿‡æ»¤åå·¥å…·æ•°: {len(filtered_tools)}")
            
            # æŒ‰LobeHubè¯„åˆ†æ’åºï¼ˆä¼˜è´¨å·¥å…·ä¼˜å…ˆï¼‰
            def sort_key(tool):
                if tool.lobehub_evaluate == 'ä¼˜è´¨':
                    return (0, -(tool.lobehub_score or 0))
                elif tool.lobehub_evaluate == 'è‰¯å¥½':
                    return (1, -(tool.lobehub_score or 0))
                else:
                    return (2, -(tool.lobehub_score or 0))
            
            filtered_tools.sort(key=sort_key)
            
            # é™åˆ¶æ•°é‡
            test_tools = filtered_tools[:test_limit]
            
            print(f"ğŸ“‹ æœ€ç»ˆæµ‹è¯•å·¥å…·æ•°: {len(test_tools)}")
            
            # ç”Ÿæˆæµ‹è¯•ç›®æ ‡æ–‡ä»¶
            with open('stress_test_targets.txt', 'w') as f:
                for tool in test_tools:
                    # æ ¼å¼ï¼šåŒ…å|å·¥å…·å|è¯„çº§|è¯„åˆ†|Stars
                    f.write(f"{tool.package_name}|{tool.name}|{tool.lobehub_evaluate or 'N/A'}|{tool.lobehub_score or 0}|{tool.lobehub_star_count or 0}\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            quality_count = sum(1 for t in test_tools if t.lobehub_evaluate == 'ä¼˜è´¨')
            good_count = sum(1 for t in test_tools if t.lobehub_evaluate == 'è‰¯å¥½')
            other_count = len(test_tools) - quality_count - good_count
            
            print(f"ğŸ“Š æµ‹è¯•å·¥å…·è´¨é‡åˆ†å¸ƒ:")
            print(f"  - ä¼˜è´¨: {quality_count} ä¸ª")
            print(f"  - è‰¯å¥½: {good_count} ä¸ª")
            print(f"  - å…¶ä»–: {other_count} ä¸ª")
            
            return len(test_tools)
        
        if __name__ == "__main__":
            count = main()
            print(f"target_count={count}")
        EOF
        
        export TEST_LIMIT="$TEST_LIMIT"
        export DEPLOYMENT_METHOD="$DEPLOYMENT_METHOD"
        export CATEGORY_FILTER="$CATEGORY_FILTER"
        
        TARGET_COUNT=$(uv run python generate_targets.py | grep "target_count=" | cut -d= -f2)
        echo "target_count=$TARGET_COUNT" >> $GITHUB_OUTPUT
        
        if [ "$TARGET_COUNT" -eq 0 ]; then
          echo "âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æµ‹è¯•ç›®æ ‡"
          exit 1
        fi
        
        echo "âœ… ç”Ÿæˆäº† $TARGET_COUNT ä¸ªæµ‹è¯•ç›®æ ‡"
    
    - name: ğŸš€ Execute Stress Testing
      id: stress_test
      run: |
        echo "ğŸš€ å¼€å§‹MCPå·¥å…·å‹åŠ›æµ‹è¯•..."
        
        # é…ç½®å‚æ•°
        TIMEOUT_PER_TOOL="${{ github.event.inputs.timeout_per_tool || '300' }}"
        SMART_MODE="${{ github.event.inputs.smart_mode || 'true' }}"
        TARGET_COUNT="${{ steps.generate_targets.outputs.target_count }}"
        
        echo "âš™ï¸ æµ‹è¯•é…ç½®:"
        echo "  - ç›®æ ‡æ•°é‡: $TARGET_COUNT"
        echo "  - å•å·¥å…·è¶…æ—¶: ${TIMEOUT_PER_TOOL}s"
        echo "  - æ™ºèƒ½æµ‹è¯•: $SMART_MODE"
        
        # è®¾ç½®æ™ºèƒ½æµ‹è¯•å‚æ•°
        if [ "$SMART_MODE" = "true" ] && ([ -n "$OPENAI_API_KEY" ] || [ -n "$DASHSCOPE_API_KEY" ]); then
          echo "âœ… AIé…ç½®æ£€æµ‹æˆåŠŸï¼Œå¯ç”¨æ™ºèƒ½æµ‹è¯•"
          SMART_FLAG=""
        else
          echo "âš ï¸ ç¦ç”¨æ™ºèƒ½æµ‹è¯•ï¼ˆé…ç½®ç¼ºå¤±æˆ–ç”¨æˆ·é€‰æ‹©ï¼‰"
          SMART_FLAG="--no-smart"
        fi
        
        # æ£€æŸ¥æ•°æ®åº“é…ç½®
        if [ -n "$SUPABASE_URL" ] && [ -n "$SUPABASE_SERVICE_ROLE_KEY" ]; then
          echo "âœ… æ•°æ®åº“é…ç½®æ£€æµ‹æˆåŠŸï¼Œå¯ç”¨æ•°æ®å¯¼å‡º"
          DB_FLAG=""
        else
          echo "âš ï¸ ç¦ç”¨æ•°æ®åº“å¯¼å‡ºï¼ˆé…ç½®ç¼ºå¤±ï¼‰"
          DB_FLAG="--no-db-export"
        fi
        
        # æ‰§è¡Œå‹åŠ›æµ‹è¯•
        cat > stress_test.py << 'EOF'
        #!/usr/bin/env python3
        import sys
        import os
        import time
        import subprocess
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from datetime import datetime
        
        sys.path.insert(0, '.')
        
        def test_single_tool(package_info, timeout, smart_flag, db_flag):
            """æµ‹è¯•å•ä¸ªå·¥å…·"""
            parts = package_info.strip().split('|')
            package_name = parts[0]
            tool_name = parts[1]
            quality = parts[2]
            score = parts[3]
            stars = parts[4]
            
            start_time = time.time()
            
            try:
                print(f"ğŸ§ª å¼€å§‹æµ‹è¯•: {tool_name} ({package_name}) - {quality}")
                
                # æ„å»ºæµ‹è¯•å‘½ä»¤ - evaluateæ¨¡å¼é»˜è®¤å¯ç”¨
                cmd = [
                    'uv', 'run', 'python', '-m', 'src.main',
                    'test-package', package_name,
                    '--timeout', str(timeout),
                    '--verbose'
                ]
                
                if smart_flag:
                    cmd.append(smart_flag)
                if db_flag:
                    cmd.append(db_flag)
                
                # æ‰§è¡Œæµ‹è¯•
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=timeout + 30  # é¢å¤–30ç§’ç¼“å†²
                )
                
                duration = time.time() - start_time
                success = result.returncode == 0
                
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': success,
                    'duration': duration,
                    'error': result.stderr if not success else None
                }
                
            except subprocess.TimeoutExpired:
                duration = time.time() - start_time
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': False,
                    'duration': duration,
                    'error': 'TIMEOUT'
                }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': False,
                    'duration': duration,
                    'error': str(e)
                }
        
        def main():
            # ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
            timeout = int(os.getenv('TIMEOUT_PER_TOOL', 300))
            smart_flag = os.getenv('SMART_FLAG', '')
            db_flag = os.getenv('DB_FLAG', '')
            
            # è¯»å–æµ‹è¯•ç›®æ ‡
            with open('stress_test_targets.txt', 'r') as f:
                targets = f.readlines()
            
            print(f"ğŸš€ å¼€å§‹å‹åŠ›æµ‹è¯• {len(targets)} ä¸ªå·¥å…·")
            print(f"âš™ï¸ é…ç½®: è¶…æ—¶{timeout}s, æ™ºèƒ½æµ‹è¯•{'å¯ç”¨' if not smart_flag else 'ç¦ç”¨'}")
            
            # å¹¶è¡Œæµ‹è¯• (æœ€å¤š3ä¸ªå¹¶å‘)
            results = []
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {
                    executor.submit(test_single_tool, target, timeout, smart_flag, db_flag): target
                    for target in targets
                }
                
                completed = 0
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    status = "âœ… æˆåŠŸ" if result['success'] else "âŒ å¤±è´¥"
                    print(f"[{completed}/{len(targets)}] {status} {result['tool_name']} ({result['duration']:.1f}s)")
            
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            successful = sum(1 for r in results if r['success'])
            failed = len(results) - successful
            total_duration = sum(r['duration'] for r in results)
            avg_duration = total_duration / len(results) if results else 0
            
            print(f"\nğŸ“Š å‹åŠ›æµ‹è¯•å®Œæˆ:")
            print(f"  - æ€»æ•°: {len(results)}")
            print(f"  - æˆåŠŸ: {successful} ({successful/len(results)*100:.1f}%)")
            print(f"  - å¤±è´¥: {failed} ({failed/len(results)*100:.1f}%)")
            print(f"  - æ€»è€—æ—¶: {total_duration:.1f}s")
            print(f"  - å¹³å‡è€—æ—¶: {avg_duration:.1f}s")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            with open('stress_test_results.txt', 'w') as f:
                f.write(f"å‹åŠ›æµ‹è¯•ç»“æœ - {datetime.now()}\n")
                f.write(f"æ€»æ•°:{len(results)} æˆåŠŸ:{successful} å¤±è´¥:{failed}\n")
                f.write(f"æˆåŠŸç‡:{successful/len(results)*100:.1f}% æ€»è€—æ—¶:{total_duration:.1f}s\n\n")
                
                for result in results:
                    status = "SUCCESS" if result['success'] else "FAILED"
                    f.write(f"{status} | {result['tool_name']} | {result['package_name']} | "
                           f"{result['quality']} | {result['duration']:.1f}s\n")
                    if result['error']:
                        f.write(f"  ERROR: {result['error']}\n")
            
            return successful, failed
        
        if __name__ == "__main__":
            success_count, fail_count = main()
            print(f"success_count={success_count}")
            print(f"fail_count={fail_count}")
        EOF
        
        export TIMEOUT_PER_TOOL="$TIMEOUT_PER_TOOL"
        export SMART_FLAG="$SMART_FLAG"
        export DB_FLAG="$DB_FLAG"
        
        # æ‰§è¡Œå‹åŠ›æµ‹è¯•
        RESULTS=$(uv run python stress_test.py | tail -2)
        SUCCESS_COUNT=$(echo "$RESULTS" | grep "success_count=" | cut -d= -f2)
        FAIL_COUNT=$(echo "$RESULTS" | grep "fail_count=" | cut -d= -f2)
        
        echo "success_count=$SUCCESS_COUNT" >> $GITHUB_OUTPUT
        echo "fail_count=$FAIL_COUNT" >> $GITHUB_OUTPUT
        echo "total_count=$TARGET_COUNT" >> $GITHUB_OUTPUT
        
        # è®¡ç®—æˆåŠŸç‡
        if [ "$TARGET_COUNT" -gt 0 ]; then
          SUCCESS_RATE=$(echo "scale=1; $SUCCESS_COUNT * 100 / $TARGET_COUNT" | bc -l)
          echo "success_rate=${SUCCESS_RATE}%" >> $GITHUB_OUTPUT
        else
          echo "success_rate=0%" >> $GITHUB_OUTPUT
        fi
    
    - name: ğŸ“‹ Generate Stress Test Report
      run: |
        echo "ğŸ“‹ ç”Ÿæˆå‹åŠ›æµ‹è¯•æ±‡æ€»æŠ¥å‘Š..."
        
        SUCCESS_COUNT="${{ steps.stress_test.outputs.success_count }}"
        FAIL_COUNT="${{ steps.stress_test.outputs.fail_count }}"
        TOTAL_COUNT="${{ steps.stress_test.outputs.total_count }}"
        SUCCESS_RATE="${{ steps.stress_test.outputs.success_rate }}"
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        cat > stress_test_summary.md << EOF
        # ğŸš€ MCP Tools Stress Test Report
                
        **æµ‹è¯•æ—¶é—´**: \$(date)  
        **GitHub Action**: [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
        
        ## ğŸ“Š æµ‹è¯•æ±‡æ€»
        
        - ğŸ¯ æµ‹è¯•æ€»æ•°: \$TOTAL_COUNT
        - âœ… æˆåŠŸæ•°é‡: \$SUCCESS_COUNT
        - âŒ å¤±è´¥æ•°é‡: \$FAIL_COUNT
        - ğŸ“ˆ æˆåŠŸç‡: \$SUCCESS_RATE
        
        ## âš™ï¸ æµ‹è¯•é…ç½®
        
        - **éƒ¨ç½²æ–¹å¼**: ${{ github.event.inputs.deployment_method || 'npx' }}
        - **å•å·¥å…·è¶…æ—¶**: ${{ github.event.inputs.timeout_per_tool || '300' }}s
        - **æ™ºèƒ½æµ‹è¯•**: ${{ github.event.inputs.smart_mode || 'true' }}
        - **ç±»åˆ«è¿‡æ»¤**: ${{ github.event.inputs.category_filter || 'æ— ' }}
        
        ## ğŸ“„ è¯¦ç»†ç»“æœ
        
        è¯¦è§å‹åŠ›æµ‹è¯•ç»“æœæ–‡ä»¶ \`stress_test_results.txt\`
        
        ---
        **Generated by GitHub Actions** ğŸ¤–
        EOF
        
        echo "ğŸ“„ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ"
        
    - name: ğŸ“¤ Upload Test Results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: |
          stress_test_*.txt
          stress_test_*.md
          data/test_results/
        retention-days: 30
        
    - name: ğŸŒ Setup GitHub Pages
      if: always()
      uses: actions/configure-pages@v3
      
    - name: ğŸ“„ Prepare Pages Content
      run: |
        mkdir -p _site
        
        # å¤åˆ¶æ±‡æ€»æŠ¥å‘Š
        if [ -f "stress_test_summary.md" ]; then
          cp stress_test_summary.md _site/README.md
        fi
        
        # å¤åˆ¶è¯¦ç»†ç»“æœ
        if [ -f "stress_test_results.txt" ]; then
          cp stress_test_results.txt _site/
        fi
        
        # å¤åˆ¶æµ‹è¯•æŠ¥å‘Š
        if [ -d "data/test_results" ]; then
          cp -r data/test_results _site/
        fi
        
        echo "ğŸ“„ GitHub Pageså†…å®¹å‡†å¤‡å®Œæˆ"
        
    - name: ğŸš€ Deploy to GitHub Pages
      if: always()
      id: deployment
      uses: actions/deploy-pages@v2
      with:
        path: _site
