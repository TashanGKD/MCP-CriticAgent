name: 🚀 MCP Tools Stress Testing

on:
  workflow_dispatch:
    inputs:
      test_limit:
        description: '测试工具数量限制'
        required: false
        default: '50'
        type: string
      deployment_method:
        description: '部署方式过滤'
        required: false
        default: 'npx'
        type: choice
        options:
        - 'npx'
        - 'pip'
        - 'all'
      timeout_per_tool:
        description: '单个工具测试超时时间（秒）'
        required: false
        default: '300'
        type: string
      smart_mode:
        description: '启用AI智能测试'
        required: false
        default: 'true'
        type: boolean
      category_filter:
        description: '按类别过滤（可选）'
        required: false
        default: ''
        type: string
  schedule:
    # 每周日凌晨2点自动执行压测（UTC时间）
    - cron: '0 2 * * 0'

jobs:
  stress-test:
    permissions:
      contents: read
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    
    env:
      # AI模型配置
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      OPENAI_BASE_URL: ${{ secrets.OPENAI_BASE_URL }}
      OPENAI_MODEL: ${{ secrets.OPENAI_MODEL }}
      DASHSCOPE_API_KEY: ${{ secrets.DASHSCOPE_API_KEY }}
      DASHSCOPE_BASE_URL: ${{ secrets.DASHSCOPE_BASE_URL }}
      DASHSCOPE_MODEL: ${{ secrets.DASHSCOPE_MODEL }}
      
      # Supabase数据库配置
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '22'
        
    - name: Install UV
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.local/bin" >> $GITHUB_PATH
      
    - name: Install dependencies
      run: uv sync
    
    - name: 🎯 Generate Test Target List
      id: generate_targets
      run: |
        echo "🔍 生成压测目标列表..."
        
        # 设置参数
        TEST_LIMIT="${{ github.event.inputs.test_limit || '50' }}"
        DEPLOYMENT_METHOD="${{ github.event.inputs.deployment_method || 'npx' }}"
        CATEGORY_FILTER="${{ github.event.inputs.category_filter || '' }}"
        
        echo "📊 压测配置:"
        echo "  - 测试限制: $TEST_LIMIT 个工具"
        echo "  - 部署方式: $DEPLOYMENT_METHOD"
        echo "  - 类别过滤: ${CATEGORY_FILTER:-'无'}"
        
        # 生成测试目标列表
        cat > generate_targets.py << 'EOF'
        #!/usr/bin/env python3
        import sys
        import os
        sys.path.insert(0, '.')
        
        from src.utils.csv_parser import get_mcp_parser
        
        def main():
            parser = get_mcp_parser()
            tools = parser.get_all_tools()
            
            # 从环境变量获取参数
            test_limit = int(os.getenv('TEST_LIMIT', 50))
            deployment_method = os.getenv('DEPLOYMENT_METHOD', 'npx').lower()
            category_filter = os.getenv('CATEGORY_FILTER', '').lower()
            
            print(f"📦 总工具数: {len(tools)}")
            
            # 过滤条件
            filtered_tools = []
            for tool in tools:
                # 部署方式过滤
                if deployment_method != 'all':
                    if deployment_method == 'npx' and tool.deployment_method != 'npx':
                        continue
                    elif deployment_method == 'pip' and tool.deployment_method != 'pip':
                        continue
                
                # 类别过滤
                if category_filter and category_filter not in tool.category.lower():
                    continue
                
                # 必须有包名才能测试
                if not tool.package_name:
                    continue
                    
                filtered_tools.append(tool)
            
            print(f"🎯 过滤后工具数: {len(filtered_tools)}")
            
            # 按LobeHub评分排序（优质工具优先）
            def sort_key(tool):
                if tool.lobehub_evaluate == '优质':
                    return (0, -(tool.lobehub_score or 0))
                elif tool.lobehub_evaluate == '良好':
                    return (1, -(tool.lobehub_score or 0))
                else:
                    return (2, -(tool.lobehub_score or 0))
            
            filtered_tools.sort(key=sort_key)
            
            # 限制数量
            test_tools = filtered_tools[:test_limit]
            
            print(f"📋 最终测试工具数: {len(test_tools)}")
            
            # 生成测试目标文件
            with open('stress_test_targets.txt', 'w') as f:
                for tool in test_tools:
                    # 格式：包名|工具名|评级|评分|Stars
                    f.write(f"{tool.package_name}|{tool.name}|{tool.lobehub_evaluate or 'N/A'}|{tool.lobehub_score or 0}|{tool.lobehub_star_count or 0}\n")
            
            # 统计信息
            quality_count = sum(1 for t in test_tools if t.lobehub_evaluate == '优质')
            good_count = sum(1 for t in test_tools if t.lobehub_evaluate == '良好')
            other_count = len(test_tools) - quality_count - good_count
            
            print(f"📊 测试工具质量分布:")
            print(f"  - 优质: {quality_count} 个")
            print(f"  - 良好: {good_count} 个")
            print(f"  - 其他: {other_count} 个")
            
            return len(test_tools)
        
        if __name__ == "__main__":
            count = main()
            print(f"target_count={count}")
        EOF
        
        export TEST_LIMIT="$TEST_LIMIT"
        export DEPLOYMENT_METHOD="$DEPLOYMENT_METHOD"
        export CATEGORY_FILTER="$CATEGORY_FILTER"
        
        TARGET_COUNT=$(uv run python generate_targets.py | grep "target_count=" | cut -d= -f2)
        echo "target_count=$TARGET_COUNT" >> $GITHUB_OUTPUT
        
        if [ "$TARGET_COUNT" -eq 0 ]; then
          echo "❌ 没有找到符合条件的测试目标"
          exit 1
        fi
        
        echo "✅ 生成了 $TARGET_COUNT 个测试目标"
    
    - name: 🚀 Execute Stress Testing
      id: stress_test
      run: |
        echo "🚀 开始MCP工具压力测试..."
        
        # 配置参数
        TIMEOUT_PER_TOOL="${{ github.event.inputs.timeout_per_tool || '300' }}"
        SMART_MODE="${{ github.event.inputs.smart_mode || 'true' }}"
        TARGET_COUNT="${{ steps.generate_targets.outputs.target_count }}"
        
        echo "⚙️ 测试配置:"
        echo "  - 目标数量: $TARGET_COUNT"
        echo "  - 单工具超时: ${TIMEOUT_PER_TOOL}s"
        echo "  - 智能测试: $SMART_MODE"
        
        # 设置智能测试参数
        if [ "$SMART_MODE" = "true" ] && ([ -n "$OPENAI_API_KEY" ] || [ -n "$DASHSCOPE_API_KEY" ]); then
          echo "✅ AI配置检测成功，启用智能测试"
          SMART_FLAG=""
        else
          echo "⚠️ 禁用智能测试（配置缺失或用户选择）"
          SMART_FLAG="--no-smart"
        fi
        
        # 检查数据库配置
        if [ -n "$SUPABASE_URL" ] && [ -n "$SUPABASE_SERVICE_ROLE_KEY" ]; then
          echo "✅ 数据库配置检测成功，启用数据导出"
          DB_FLAG=""
        else
          echo "⚠️ 禁用数据库导出（配置缺失）"
          DB_FLAG="--no-db-export"
        fi
        
        # 执行压力测试
        cat > stress_test.py << 'EOF'
        #!/usr/bin/env python3
        import sys
        import os
        import time
        import subprocess
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from datetime import datetime
        
        sys.path.insert(0, '.')
        
        def test_single_tool(package_info, timeout, smart_flag, db_flag):
            """测试单个工具"""
            parts = package_info.strip().split('|')
            package_name = parts[0]
            tool_name = parts[1]
            quality = parts[2]
            score = parts[3]
            stars = parts[4]
            
            start_time = time.time()
            
            try:
                print(f"🧪 开始测试: {tool_name} ({package_name}) - {quality}")
                
                # 构建测试命令 - evaluate模式默认启用
                cmd = [
                    'uv', 'run', 'python', '-m', 'src.main',
                    'test-package', package_name,
                    '--timeout', str(timeout),
                    '--verbose'
                ]
                
                if smart_flag:
                    cmd.append(smart_flag)
                if db_flag:
                    cmd.append(db_flag)
                
                # 执行测试
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=timeout + 30  # 额外30秒缓冲
                )
                
                duration = time.time() - start_time
                success = result.returncode == 0
                
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': success,
                    'duration': duration,
                    'error': result.stderr if not success else None
                }
                
            except subprocess.TimeoutExpired:
                duration = time.time() - start_time
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': False,
                    'duration': duration,
                    'error': 'TIMEOUT'
                }
            except Exception as e:
                duration = time.time() - start_time
                return {
                    'package_name': package_name,
                    'tool_name': tool_name,
                    'quality': quality,
                    'score': score,
                    'stars': stars,
                    'success': False,
                    'duration': duration,
                    'error': str(e)
                }
        
        def main():
            # 从环境变量获取参数
            timeout = int(os.getenv('TIMEOUT_PER_TOOL', 300))
            smart_flag = os.getenv('SMART_FLAG', '')
            db_flag = os.getenv('DB_FLAG', '')
            
            # 读取测试目标
            with open('stress_test_targets.txt', 'r') as f:
                targets = f.readlines()
            
            print(f"🚀 开始压力测试 {len(targets)} 个工具")
            print(f"⚙️ 配置: 超时{timeout}s, 智能测试{'启用' if not smart_flag else '禁用'}")
            
            # 并行测试 (最多3个并发)
            results = []
            with ThreadPoolExecutor(max_workers=3) as executor:
                futures = {
                    executor.submit(test_single_tool, target, timeout, smart_flag, db_flag): target
                    for target in targets
                }
                
                completed = 0
                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    status = "✅ 成功" if result['success'] else "❌ 失败"
                    print(f"[{completed}/{len(targets)}] {status} {result['tool_name']} ({result['duration']:.1f}s)")
            
            # 生成汇总报告
            successful = sum(1 for r in results if r['success'])
            failed = len(results) - successful
            total_duration = sum(r['duration'] for r in results)
            avg_duration = total_duration / len(results) if results else 0
            
            print(f"\n📊 压力测试完成:")
            print(f"  - 总数: {len(results)}")
            print(f"  - 成功: {successful} ({successful/len(results)*100:.1f}%)")
            print(f"  - 失败: {failed} ({failed/len(results)*100:.1f}%)")
            print(f"  - 总耗时: {total_duration:.1f}s")
            print(f"  - 平均耗时: {avg_duration:.1f}s")
            
            # 保存详细结果
            with open('stress_test_results.txt', 'w') as f:
                f.write(f"压力测试结果 - {datetime.now()}\n")
                f.write(f"总数:{len(results)} 成功:{successful} 失败:{failed}\n")
                f.write(f"成功率:{successful/len(results)*100:.1f}% 总耗时:{total_duration:.1f}s\n\n")
                
                for result in results:
                    status = "SUCCESS" if result['success'] else "FAILED"
                    f.write(f"{status} | {result['tool_name']} | {result['package_name']} | "
                           f"{result['quality']} | {result['duration']:.1f}s\n")
                    if result['error']:
                        f.write(f"  ERROR: {result['error']}\n")
            
            return successful, failed
        
        if __name__ == "__main__":
            success_count, fail_count = main()
            print(f"success_count={success_count}")
            print(f"fail_count={fail_count}")
        EOF
        
        export TIMEOUT_PER_TOOL="$TIMEOUT_PER_TOOL"
        export SMART_FLAG="$SMART_FLAG"
        export DB_FLAG="$DB_FLAG"
        
        # 执行压力测试
        RESULTS=$(uv run python stress_test.py | tail -2)
        SUCCESS_COUNT=$(echo "$RESULTS" | grep "success_count=" | cut -d= -f2)
        FAIL_COUNT=$(echo "$RESULTS" | grep "fail_count=" | cut -d= -f2)
        
        echo "success_count=$SUCCESS_COUNT" >> $GITHUB_OUTPUT
        echo "fail_count=$FAIL_COUNT" >> $GITHUB_OUTPUT
        echo "total_count=$TARGET_COUNT" >> $GITHUB_OUTPUT
        
        # 计算成功率
        if [ "$TARGET_COUNT" -gt 0 ]; then
          SUCCESS_RATE=$(echo "scale=1; $SUCCESS_COUNT * 100 / $TARGET_COUNT" | bc -l)
          echo "success_rate=${SUCCESS_RATE}%" >> $GITHUB_OUTPUT
        else
          echo "success_rate=0%" >> $GITHUB_OUTPUT
        fi
    
    - name: 📋 Generate Stress Test Report
      run: |
        echo "📋 生成压力测试汇总报告..."
        
        SUCCESS_COUNT="${{ steps.stress_test.outputs.success_count }}"
        FAIL_COUNT="${{ steps.stress_test.outputs.fail_count }}"
        TOTAL_COUNT="${{ steps.stress_test.outputs.total_count }}"
        SUCCESS_RATE="${{ steps.stress_test.outputs.success_rate }}"
        
        # 生成Markdown报告
        cat > stress_test_summary.md << EOF
        # 🚀 MCP Tools Stress Test Report
                
        **测试时间**: \$(date)  
        **GitHub Action**: [#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
        
        ## 📊 测试汇总
        
        - 🎯 测试总数: \$TOTAL_COUNT
        - ✅ 成功数量: \$SUCCESS_COUNT
        - ❌ 失败数量: \$FAIL_COUNT
        - 📈 成功率: \$SUCCESS_RATE
        
        ## ⚙️ 测试配置
        
        - **部署方式**: ${{ github.event.inputs.deployment_method || 'npx' }}
        - **单工具超时**: ${{ github.event.inputs.timeout_per_tool || '300' }}s
        - **智能测试**: ${{ github.event.inputs.smart_mode || 'true' }}
        - **类别过滤**: ${{ github.event.inputs.category_filter || '无' }}
        
        ## 📄 详细结果
        
        详见压力测试结果文件 \`stress_test_results.txt\`
        
        ---
        **Generated by GitHub Actions** 🤖
        EOF
        
        echo "📄 汇总报告已生成"
        
    - name: 📤 Upload Test Results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: |
          stress_test_*.txt
          stress_test_*.md
          data/test_results/
        retention-days: 30
        
    - name: 🌐 Setup GitHub Pages
      if: always()
      uses: actions/configure-pages@v3
      
    - name: 📄 Prepare Pages Content
      run: |
        mkdir -p _site
        
        # 复制汇总报告
        if [ -f "stress_test_summary.md" ]; then
          cp stress_test_summary.md _site/README.md
        fi
        
        # 复制详细结果
        if [ -f "stress_test_results.txt" ]; then
          cp stress_test_results.txt _site/
        fi
        
        # 复制测试报告
        if [ -d "data/test_results" ]; then
          cp -r data/test_results _site/
        fi
        
        echo "📄 GitHub Pages内容准备完成"
        
    - name: 🚀 Deploy to GitHub Pages
      if: always()
      id: deployment
      uses: actions/deploy-pages@v2
      with:
        path: _site
